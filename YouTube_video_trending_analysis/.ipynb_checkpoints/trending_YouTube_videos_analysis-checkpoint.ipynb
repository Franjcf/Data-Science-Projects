{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis of Trending YouTube Videos Across the USA, Great Britain, Canada and Mexico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project is on the public domain and was obtained from ”Kaggle.com”. The data itself contains trending video information from the USA, Great Britain, Germany, Canada, France,Russia, Mexico, South Korea, Japan, and India.  Each data set contains a dated list (spanning 200 days) of the top 200 trending videos for each country,  as well as 16 additional features such as:the video’s title, its description (written by the videos author), video ‘tags’, URL, number of likes,dislikes, and views, a generic category ID, and upload date. This means that videos that stay on thetrending list over several days appear repeatedly on the data, once on every daily trending list. The original data set size for each country is about 40000 by 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the main libraries\n",
    "import warnings #prevent \"future warning\" errors\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "#Getting Stop words for languange analysis\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stopEs = stopwords.words('spanish')\n",
    "\n",
    "#Importing K folds\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Loading latent analysis models\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#loading Regression Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, roc_auc_score,average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Main Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA function\n",
    "def LDA(NComponents,dataSource):\n",
    "    \"\"\"Function for performing LDA, returns data organized into NComponents\"\"\"\n",
    "    \n",
    "    model = LatentDirichletAllocation(n_components = NComponents) \n",
    "    model.fit(dataSource.values)\n",
    "    out = model.transform(dataSource.values)\n",
    "    components = model.components_\n",
    "    print (\"Transformed Size: \", out.shape)\n",
    "    print (\"Component Size: \", components.shape)\n",
    "\n",
    "    return out,components\n",
    "\n",
    "##Choosing LDA's Nth topic and analyzing the features within top chosen percentile\n",
    "def topicDescription(dataSource,components,topic,percentile):\n",
    "    \"\"\"\n",
    "    This function gets the LDA-transformed data as an input and \n",
    "    outputs an analysis of the features within top chosen percentile of the Nth topic.\n",
    "    \"\"\"\n",
    "    topicData = components.iloc[topic,:] \n",
    "    indices = np.where(topicData > np.percentile(topicData, percentile))\n",
    "    \n",
    "    topTopic = pd.DataFrame({'Feature': dataSource.columns.values[indices],('Distribution of Topic #'+str(topic+1)): topicData.iloc[indices]}) #if from csv file\n",
    "    topTopic.sort_values(('Distribution of Topic #'+str(topic+1)), inplace=True, ascending=False)\n",
    "    topTopic.head(n=80)\n",
    "    return topTopic\n",
    "\n",
    "##Choosing the users that most belog to each topic\n",
    "def videoDescription(dataSource,transformed,topic,percentile):\n",
    "    \"\"\"\n",
    "    This function gets the LDA-transformed data as an input and \n",
    "    outputs an analysis of the users within the Nth topic.\n",
    "    \"\"\"\n",
    "    videoData = transformed[topic]  \n",
    "    dummyData = dataSource.copy()\n",
    "    dummyData.insert(0, (\"Weight of topic #\"+ str(topic+1)), videoData)\n",
    "    dummyData.sort_values((\"Weight of topic #\"+ str(topic+1)), inplace=True, ascending=False)\n",
    "\n",
    "    return dummyData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Proccesing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Categories To Countries (changing category ID numbers to actual category name)\n",
    "def assignCategories(data):\n",
    "    data['category_id'].replace(1,'Film and Animation',inplace=True)\n",
    "    data['category_id'].replace(2,'Vehicles',inplace=True)\n",
    "    data['category_id'].replace(10,'Music',inplace=True)\n",
    "    data['category_id'].replace(15,'Pets and Animals',inplace=True)\n",
    "    data['category_id'].replace(17,'Sports',inplace=True)\n",
    "    data['category_id'].replace(18,'Short Movies',inplace=True)\n",
    "    data['category_id'].replace(19,'Travel and Events',inplace=True)\n",
    "    data['category_id'].replace(20,'Gaming',inplace=True)\n",
    "    data['category_id'].replace(22,'Peaple and Blogs',inplace=True)\n",
    "    data['category_id'].replace(23,'Comedy',inplace=True)\n",
    "    data['category_id'].replace(24,'Entertainment',inplace=True)\n",
    "    data['category_id'].replace(25,'News and Politics',inplace=True)\n",
    "    data['category_id'].replace(26,'Style',inplace=True)\n",
    "    data['category_id'].replace(27,'Education',inplace=True)\n",
    "    data['category_id'].replace(28,'Science and Tech.',inplace=True)\n",
    "    data['category_id'].replace(29,'Activism and Nonprofits',inplace=True)\n",
    "    data['category_id'].replace(30,'Movies',inplace=True)\n",
    "    data['category_id'].replace(43,'Shows',inplace=True)\n",
    "\n",
    "# create numerical, time, and one-hot encoded features\n",
    "#(one hot enconding categories and calculating continous variables from likes and views)\n",
    "def makeFeatures(data):\n",
    "    dummy = data\n",
    "    dummy['percent_likes'] = 100*dummy['likes']/dummy['views']\n",
    "    dummy['percent_dislikes'] = 100*dummy['dislikes']/dummy['views']\n",
    "    dummy['percent_comments'] = 100*dummy['comment_count']/dummy['views']\n",
    "    dummy['likes/dislikes']   = 100*dummy['likes']/(dummy['dislikes']+1)\n",
    "    dummy['likes/comments']   = 100*dummy['likes']/(dummy['comment_count']+1)\n",
    "\n",
    "    \n",
    "    ##add published day and time \n",
    "    dummy[\"day_published\"] = dummy[\"publish_time\"].apply(lambda x: datetime.datetime.strptime(x[:10], \"%Y-%m-%d\").date().strftime('%a'))\n",
    "    dummy[\"hour_published\"] = dummy[\"publish_time\"].apply(lambda x: x[11:13])\n",
    "    dummy.drop(labels='publish_time', axis=1, inplace=True)\n",
    "    \n",
    "    #doing one-hot encoding on the categories\n",
    "    dummy = pd.get_dummies(dummy, columns =['category_id'] ) #)= list(dummy['category_id'].unique()))\n",
    "\n",
    "    return dummy\n",
    "\n",
    "# Procces data in order to make sure we have a complete data set (elimante NA and bad values\n",
    "def correlationData(data):\n",
    "\n",
    "    #drop unused \n",
    "    dataSimple = data.drop(['percent_comments','hour_published','day_published','title','tags','video_id','thumbnail_link', 'comments_disabled','ratings_disabled','video_error_or_removed','description',], axis=1)\n",
    "    \n",
    "    # Look at correlations between features (from precept 6)\n",
    "    plt.figure(figsize = (5,5), dpi=200)\n",
    "    plt.matshow(dataSimple.corr(), fignum=1, cmap=plt.cm.bwr)\n",
    "    cols = list(dataSimple.columns)\n",
    "    plt.xticks(list(range(len(cols))), cols, rotation=90)\n",
    "    plt.yticks(list(range(len(cols))), cols)\n",
    "    plt.colorbar()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count individual words within the video descriptions\n",
    "def countWords(data,name):\n",
    "    \"\"\"\n",
    "    Input: the data for a particular country\n",
    "    Output: a file named \"name\" with a list of all the words within the video descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    #concotenating all the essays into 1\n",
    "    combinedStringCols = data[\"title\"].map(str)+ data[\"tags\"].map(str) + data[\"description\"].map(str)\n",
    "\n",
    "    #removing stop-words\n",
    "    combinedStringCols = combinedStringCols.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    combinedStringCols = combinedStringCols.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopEs)]))\n",
    "\n",
    "    ##separating into individual strings/words\n",
    "    words = combinedStringCols.str.split(\" \",n=-1,expand=True)\n",
    "    \n",
    "    ##Counting the number of times each word is said\n",
    "    wordValueCounts = pd.value_counts(words.values.flatten())\n",
    "\n",
    "    #writing a text file withh the word counts\n",
    "    wordValueCounts = wordValueCounts.reset_index(level=0, inplace=False)\n",
    "    n=0\n",
    "    with open(name, 'w') as f:\n",
    "        for item in wordValueCounts['index']:\n",
    "            f.write(\"%s\\n\" % wordValueCounts['index'].iloc[n])\n",
    "            n = n + 1\n",
    "            \n",
    "            \n",
    "#doing Bag of words representation function       \n",
    "def makeBOW(data,nwords,name):\n",
    "    \"\"\"\n",
    "    Input: data and number of top words we want in the BOW description\n",
    "    Output: a Bag of words file named \"name\"\n",
    "    \"\"\"\n",
    "    \n",
    "    #Choose the top \"n\" words and load them from the proccesed word list:\n",
    "    nwords = nwords\n",
    "    words = pd.read_csv(name, low_memory=False,header=None,error_bad_lines=False,nrows = nwords)\n",
    "    #print(words)\n",
    "    \n",
    "    combinedStringCols = data[\"title\"].map(str)+ data[\"tags\"].map(str) + data[\"description\"].map(str)\n",
    "    \n",
    "    ##counting fully capitalized words\n",
    "    #b = pd.DataFrame(df['A'].values.tolist()).stack().str.split(expand=True).stack().str.isupper().sum()\n",
    "    #print (b)\n",
    "\n",
    "    ##making a list out of words:\n",
    "    wordList = list(words[0])\n",
    "    #print(wordList)\n",
    "\n",
    "    #removing duplicates\n",
    "    wordList = list(dict.fromkeys(wordList))\n",
    "    #print(wordList)\n",
    "\n",
    "    ####counting words in each line and adding them as a collumn to categorical data\n",
    "    bagOfWords = data.copy()\n",
    "    for m in wordList: \n",
    "        word  = m\n",
    "        count = combinedStringCols.str.count(word) \n",
    "        bagOfWords.insert(0,word,count,allow_duplicates=True)\n",
    "    \n",
    "    #print(bagOfWords)\n",
    "    \n",
    "    return bagOfWords,wordList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating feature for total \"days trending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function that gets the number of days a video was on the list and combines all data about each unique\n",
    "#video into a single entry. (videos are listed once for every day they are in the list)\n",
    "\n",
    "def groupVideos(data):\n",
    "    dummy = data.groupby(data['title']).mean()\n",
    "    dummy.insert(0,\"days_trending\",pd.value_counts(data['title'])) #adding days trending as a collumn    \n",
    "    dummy.sort_values(\"days_trending\", inplace=True, ascending=False)\n",
    "\n",
    "    return dummy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifing Top Trending Topics Through LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA data Shape: (40949, 16)\n",
      "GB data Shape: (38916, 16)\n",
      "CA data Shape: (40881, 16)\n",
      "MX data Shape: (40451, 16)\n",
      "\n",
      "Sample of data for US-based videos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d380meD0W0M</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12T18:01:41.000Z</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>132235</td>\n",
       "      <td>1989</td>\n",
       "      <td>17518</td>\n",
       "      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I know it's been a while since we did this sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  1ZAPwfrtAFY      17.14.11   \n",
       "2  5qpjK5DgCt4      17.14.11   \n",
       "3  puqaWrEC7tY      17.14.11   \n",
       "4  d380meD0W0M      17.14.11   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22  2017-11-13T17:13:01.000Z   \n",
       "1           24  2017-11-13T07:30:00.000Z   \n",
       "2           23  2017-11-12T19:05:24.000Z   \n",
       "3           24  2017-11-13T11:00:04.000Z   \n",
       "4           24  2017-11-12T18:01:41.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3       666           2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "4      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "1              False             False                   False   \n",
       "2              False             False                   False   \n",
       "3              False             False                   False   \n",
       "4              False             False                   False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  \n",
       "2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "3  Today we find out if Link is a Nickelback amat...  \n",
       "4  I know it's been a while since we did this sho...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data for all 4 studied countries (these files are not included here but are available at Kaggele.com)\n",
    "\n",
    "dataUSA = pd.read_csv(\"Data/USvideos.csv\") # USA\n",
    "dataGB = pd.read_csv(\"Data/GBvideos.csv\") # Great Britain\n",
    "dataCA = pd.read_csv(\"Data/CAvideos.csv\") # Canada\n",
    "dataMX = pd.read_csv(\"Data/MXvideos.csv\",encoding = \"Latin-1\") # Mexico\n",
    "\n",
    "##printing shape and looking at the data\n",
    "print('USA data Shape:',dataUSA.shape)\n",
    "print('GB data Shape:',dataGB.shape)\n",
    "print('CA data Shape:',dataCA.shape)\n",
    "print('MX data Shape:',dataMX.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Sample of data for US-based videos:\")\n",
    "dataUSA.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Categories to Data (changing category ID's to actual names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning Categories to Data \n",
    "assignCategories(dataUSA)\n",
    "assignCategories(dataGB)\n",
    "assignCategories(dataCA)\n",
    "assignCategories(dataMX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot enconding categories and calculating continous variables from likes and views\n",
    "dataUSAEng=makeFeatures(dataUSA)\n",
    "dataGBEng=makeFeatures(dataGB)\n",
    "dataCAEng=makeFeatures(dataCA) \n",
    "dataMXEng=makeFeatures(dataMX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Capitalized Word Indicator to Each Column\n",
    "We want to see if capitalization has an impact on video trendability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA Eng. data Shape: (40949, 38)\n",
      "GB Eng. data Shape: (38916, 38)\n",
      "CA Eng. data Shape: (40881, 39)\n",
      "MX Eng. data Shape: (40451, 38)\n"
     ]
    }
   ],
   "source": [
    "#counting fully capitalized words and adding them to each data set\n",
    "data = dataUSAEng.copy()\n",
    "combinedStringCols = data[\"title\"].map(str)+ data[\"tags\"].map(str) + data[\"description\"].map(str)\n",
    "capitalized = pd.DataFrame(combinedStringCols.values.tolist()).stack().str.isupper()\n",
    "dataUSAEng['capitalized'] = capitalized.values\n",
    "\n",
    "data = dataGBEng.copy()\n",
    "combinedStringCols = data[\"title\"].map(str)+ data[\"tags\"].map(str) + data[\"description\"].map(str)\n",
    "capitalized = pd.DataFrame(combinedStringCols.values.tolist()).stack().str.isupper()\n",
    "dataGBEng['capitalized'] = capitalized.values\n",
    "\n",
    "data = dataCAEng.copy()\n",
    "combinedStringCols = data[\"title\"].map(str)+ data[\"tags\"].map(str) + data[\"description\"].map(str)\n",
    "capitalized = pd.DataFrame(combinedStringCols.values.tolist()).stack().str.isupper()\n",
    "dataCAEng['capitalized'] = capitalized.values\n",
    "\n",
    "data = dataMXEng.copy()\n",
    "combinedStringCols = data[\"title\"].map(str)+ data[\"tags\"].map(str) + data[\"description\"].map(str)\n",
    "capitalized = pd.DataFrame(combinedStringCols.values.tolist()).stack().str.isupper()\n",
    "dataMXEng['capitalized'] = capitalized.values\n",
    "\n",
    "#printing shape and looking at the data\n",
    "print('USA Eng. data Shape:',dataUSAEng.shape)\n",
    "print('GB Eng. data Shape:',dataGBEng.shape)\n",
    "print('CA Eng. data Shape:',dataCAEng.shape)\n",
    "print('MX Eng. data Shape:',dataMXEng.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Individual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifing individual words\n",
    "countWords(dataUSAEng,'wordCountsUSA.txt')\n",
    "countWords(dataGBEng,'wordCountsGB.txt')\n",
    "countWords(dataCAEng,'wordCountsCA.txt')\n",
    "countWords(dataMXEng,'wordCountsMX.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting into Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of USA completely proccesed data: (40949, 138)\n",
      "Size of GB completely proccesed data: (38916, 138)\n",
      "Size of CA completely proccesed data: (40881, 139)\n",
      "Size of MX completely proccesed data: (40451, 138)\n"
     ]
    }
   ],
   "source": [
    "##Convert into BOW representation\n",
    "\n",
    "nWords = 100 # number of words in BOW representation\n",
    "\n",
    "completeUSA,wordListUSA = makeBOW(dataUSAEng,nWords,'Data/wordCountsUSA.txt')\n",
    "completeGB,wordListGB = makeBOW(dataGBEng,nWords,'Data/wordCountsGB.txt')\n",
    "completeCA,wordListCA = makeBOW(dataCAEng,nWords,'Data/wordCountsCA.txt')\n",
    "completeMX,wordListMX = makeBOW(dataMXEng,nWords,'Data/wordCountsMX.txt')\n",
    "\n",
    "print('Size of USA completely proccesed data:',completeUSA.shape)\n",
    "print('Size of GB completely proccesed data:',completeGB.shape)\n",
    "print('Size of CA completely proccesed data:',completeCA.shape)\n",
    "print('Size of MX completely proccesed data:',completeMX.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data set with count-data only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of USA count data: (40949, 117)\n",
      "Size of GB count data: (38916, 117)\n",
      "Size of CA count data: (40881, 118)\n",
      "Size of MX count data: (40451, 117)\n"
     ]
    }
   ],
   "source": [
    "##Creating a data set with count-data only (i.e. integers only)\n",
    "countUSA = completeUSA.drop(['thumbnail_link', 'comments_disabled','ratings_disabled','video_error_or_removed','description','percent_likes','percent_dislikes','percent_comments','tags','title','video_id','trending_date','channel_title','trending_date','views','likes','comment_count','dislikes',\"day_published\",'hour_published','likes/dislikes','likes/comments'], axis=1)\n",
    "countGB = completeGB.drop(['thumbnail_link', 'comments_disabled','ratings_disabled','video_error_or_removed','description','percent_likes','percent_dislikes','percent_comments','tags','title','video_id','trending_date','channel_title','trending_date','views','likes','comment_count','dislikes',\"day_published\",'hour_published','likes/dislikes','likes/comments'], axis=1)\n",
    "countCA = completeCA.drop(['thumbnail_link', 'comments_disabled','ratings_disabled','video_error_or_removed','description','percent_likes','percent_dislikes','percent_comments','tags','title','video_id','trending_date','channel_title','trending_date','views','likes','comment_count','dislikes',\"day_published\",'hour_published','likes/dislikes','likes/comments'], axis=1)\n",
    "countMX = completeMX.drop(['thumbnail_link', 'comments_disabled','ratings_disabled','video_error_or_removed','description','percent_likes','percent_dislikes','percent_comments','tags','title','video_id','trending_date','channel_title','trending_date','views','likes','comment_count','dislikes',\"day_published\",'hour_published','likes/dislikes','likes/comments'], axis=1)\n",
    "\n",
    "print('Size of USA count data:',countUSA.shape)\n",
    "print('Size of GB count data:',countGB.shape)\n",
    "print('Size of CA count data:',countCA.shape)\n",
    "print('Size of MX count data:',countMX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing LDA with N=10 on the Count Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#Warning, this takes a LONG time, about 13 minutes total\n",
    "#No need to run since I have included the files in the Data directory\n",
    "\n",
    "#Number of Topics\n",
    "ntopics = 10\n",
    "\n",
    "transformedUSA,componentsUSA = LDA(ntopics,countUSA)\n",
    "transformedGB,componentsGB = LDA(ntopics,countGB)\n",
    "transformedCA,componentsCA = LDA(ntopics,countCA)\n",
    "transformedMX,componentsMX = LDA(ntopics,countMX)\n",
    "\n",
    "np.savetxt(\"LDA_transformedUSA_10.csv\", transformedUSA, delimiter=\",\")\n",
    "np.savetxt(\"LDA_componentsUSA_10.csv\", componentsUSA, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"LDA_transformedGB_10.csv\", transformedGB, delimiter=\",\")#\n",
    "np.savetxt(\"LDA_componentsGB_10.csv\", componentsGB, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"LDA_transformedCA_10.csv\", transformedCA, delimiter=\",\")\n",
    "np.savetxt(\"LDA_componentsCA_10.csv\", componentsCA, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"LDA_transformedMX_10.csv\", transformedMX, delimiter=\",\")\n",
    "np.savetxt(\"LDA_componentsMX_10.csv\", componentsMX, delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading LDA results from text files \n",
    "\n",
    "componentsUSA = pd.read_csv(\"Data/LDA_componentsUSA_10.csv\", low_memory=False,header=None) #topic by features\n",
    "transformedUSA = pd.read_csv(\"Data/LDA_transformedUSA_10.csv\", low_memory=False,header=None) #users by topic\n",
    "componentsUSA = componentsUSA/componentsUSA.sum(axis=1)[:, np.newaxis] ##normalizing components\n",
    "\n",
    "componentsGB = pd.read_csv(\"Data/LDA_componentsGB_10.csv\", low_memory=False,header=None) #topic by features\n",
    "transformedGB = pd.read_csv(\"Data/LDA_transformedGB_10.csv\", low_memory=False,header=None) #users by topic\n",
    "componentsGB = componentsGB/componentsGB.sum(axis=1)[:, np.newaxis] ##normalizing components\n",
    "\n",
    "componentsCA = pd.read_csv(\"Data/LDA_componentsCA_10.csv\", low_memory=False,header=None) #topic by features\n",
    "transformedCA = pd.read_csv(\"Data/LDA_transformedCA_10.csv\", low_memory=False,header=None) #users by topic\n",
    "componentsCA = componentsCA/componentsCA.sum(axis=1)[:, np.newaxis] ##normalizing components\n",
    "\n",
    "componentsMX = pd.read_csv(\"Data/LDA_componentsMX_10.csv\", low_memory=False,header=None) #topic by features\n",
    "transformedMX = pd.read_csv(\"Data/LDA_transformedMX_10.csv\", low_memory=False,header=None) #users by topic\n",
    "componentsMX = componentsMX/componentsMX.sum(axis=1)[:, np.newaxis] ##normalizing components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifing top words used in description in english-speaking and all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common top words in english speaking countries:\n",
      "\n",
      " ['movie', 'time', 'make', 'Follow', 'see', 'want', 'Twitter:', 'know', 'More', 'Instagram:', 'Show', 'videos', 'Late', 'Live', 'much', 'new', 'full', 'show', 'Subscribe', 'Official', 'ON', 'use', 'made', '2017', 'channel', 'THE', 'Kimmel', 'every', 'best', 'first', 'love', 'world', 'life', 'YouTube', 'Night', 'take', 'Watch', 'Jimmy', 'latest', 'get', 'Music', 'vs', 'find', 'live', '2', 'official', 'music', '2018', 'favorite', 'Facebook:', 'like', 'back'] \n",
      "\n",
      "\n",
      "Common top words in all countries:\n",
      "\n",
      " ['use', '2017', '2', 'videos', '2018', 'Twitter:', 'Facebook:', 'like', 'Instagram:', 'Music']\n"
     ]
    }
   ],
   "source": [
    "commonWordsEnglish = (list(set(wordListUSA).intersection(wordListGB,wordListCA)))\n",
    "commonWordsAll = (list(set(wordListUSA).intersection(wordListGB,wordListCA,wordListMX)))\n",
    "#print(wordListMX)\n",
    "\n",
    "print('Common top words in english speaking countries:\\n\\n',commonWordsEnglish, \"\\n\\n\")\n",
    "print('Common top words in all countries:\\n\\n',commonWordsAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyizing the top correlated features for each latent topic in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Feature  Distribution of Topic #1\n",
      "65                        show                  0.118268\n",
      "83                        Show                  0.114253\n",
      "87                        Late                  0.108224\n",
      "48                         CBS                  0.058304\n",
      "72                       Watch                  0.041590\n",
      "33                     episode                  0.036151\n",
      "2                       Follow                  0.035995\n",
      "59                           2                  0.035929\n",
      "6                        night                  0.032214\n",
      "23                       James                  0.028296\n",
      "60                        live                  0.027242\n",
      "103  category_id_Entertainment                  0.024122\n",
      "89                        This                  0.020127\n",
      "73                        full                  0.019036\n",
      "61                        time                  0.017543\n",
      "77                   Subscribe                  0.017092\n",
      "99                         new                  0.015991\n",
      "98                      videos                  0.015330\n",
      "37                       watch                  0.014780\n",
      "70                         use                  0.014045\n",
      "32                    favorite                  0.012550\n",
      "75                       music                  0.011328\n",
      "91                  Instagram:                  0.011026\n"
     ]
    }
   ],
   "source": [
    "#For brevetiy I will only print the results from the first USA-based topic\n",
    "#however feel free to explore by yourself for different topics and different countries\n",
    "\n",
    "print(topicDescription(countUSA,componentsUSA,0,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,1,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,2,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,3,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,4,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,5,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,6,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,7,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,8,80))\n",
    "#print(topicDescription(countUSA,componentsUSA,9,80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "USA trending topics can be generally labeled as: 1) Entertainment, 2) News, 3) Movies, 4) Music, 5) Fashion, 6) Late-Night Shows, 7) Lifestyle, 8) Social Media, 9) Generic Videos, and 10) Sports.\n",
    "\n",
    "Please refer to the Report for a complete list for the remaining countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Continued Video Trendability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Classifier Functions\n",
    "\n",
    "We will test if a particular video will (label =1) or will not (label=)0) trend for longer than the average trending video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining logistic regression\n",
    "def LogisticReg(data):\n",
    "    \n",
    "    ##Making boolean values of days trending more than avg. \n",
    "    dummy = data.copy()\n",
    "    mean = dummy['days_trending'].mean()\n",
    "\n",
    "    mask1 = dummy['days_trending'] >= mean\n",
    "    mask2 = dummy['days_trending'] < mean\n",
    "\n",
    "    dummy.loc[mask1, 'days_trending'] = 1\n",
    "    dummy.loc[mask2, 'days_trending'] = 0\n",
    "\n",
    "    ##separating into training and test data\n",
    "    kfold = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    result = next(kfold.split(dummy), None)\n",
    "    \n",
    "    train = dummy.iloc[result[0]]\n",
    "    trainIn = train.drop(['days_trending'],axis=1)\n",
    "    trainOut = train['days_trending']\n",
    "    \n",
    "    test =  dummy.iloc[result[1]]\n",
    "    testIn = train.drop(['days_trending'],axis=1)\n",
    "    testOut = train['days_trending']\n",
    "    \n",
    "    ##Doing logistic regression\n",
    "    log = LogisticRegression(penalty = \"l1\",solver='liblinear',max_iter=1000,C=0.1)\n",
    "    log.fit(trainIn,trainOut) \n",
    "    prediction = log.predict(testIn)\n",
    "    probs = log.predict_proba(testIn)\n",
    "    probs = probs[:, 1]\n",
    "    score = log.score(testIn,testOut)\n",
    "    coeff = log.coef_\n",
    "    print(\"Accuracy:\",\"\\n\",score)\n",
    "    print(\"Precision Score:\",average_precision_score(testOut,prediction),\"\\n\\n\")\n",
    "    print(\"ROC area under curve:\", roc_auc_score(testOut,probs))\n",
    "    print(\"Confusion Matrix:\",\"\\n\",confusion_matrix(testOut,prediction),\"\\n\")\n",
    "    \n",
    "#Recursive Feature Elimination to extract feature importances\n",
    "def RecursiveFeatureElimination(data,model,nFeatures):\n",
    "    ##separating into training and test data\n",
    "    dummy = data.copy()\n",
    "    kfold = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    result = next(kfold.split(dummy), None)\n",
    "    \n",
    "    train = dummy.iloc[result[0]]\n",
    "    trainIn = train.drop(['days_trending'],axis=1)\n",
    "    trainOut = train['days_trending']\n",
    "    \n",
    "    test =  dummy.iloc[result[1]]\n",
    "    testIn = train.drop(['days_trending'],axis=1)\n",
    "    testOut = train['days_trending']\n",
    "    \n",
    "    \n",
    "    ## Feature Selection based on Recursive Feature Elimination\n",
    "    numFeatures = nFeatures #number of words that we want to use\n",
    "\n",
    "    rfe = RFE(model,numFeatures) \n",
    "    rfe = rfe.fit(trainIn, trainOut)\n",
    "    \n",
    "    features = list(testIn.columns.values)\n",
    "    bestFeatures = []\n",
    "    num = 0 \n",
    "    \n",
    "    #choosing the best words from the rfe analysis and putting them into a dictinoary\n",
    "    for bool in rfe.support_:\n",
    "        if bool  == True:\n",
    "            bestFeatures.append(features[num]) \n",
    "        num = num+1\n",
    "        \n",
    "    n=0\n",
    "    \n",
    "    #writing a text file with most representative words\n",
    "    with open('bestFeaturesGB.txt', 'w') as f:\n",
    "        for item in bestFeatures:\n",
    "            f.write(\"%s\\n\" % bestFeatures[n])\n",
    "            n = n + 1\n",
    "    \n",
    "    print(bestFeatures) \n",
    "\n",
    "##Defining Random Forest Classifier\n",
    "def RFClassifier(data):\n",
    "    ##Making boolean values of days trending more than avg. \n",
    "    dummy = data.copy()\n",
    "    mean = dummy['days_trending'].mean()\n",
    "\n",
    "    mask1 = dummy['days_trending'] >= mean\n",
    "    mask2 = dummy['days_trending'] < mean\n",
    "\n",
    "    dummy.loc[mask1, 'days_trending'] = 1\n",
    "    dummy.loc[mask2, 'days_trending'] = 0\n",
    "\n",
    "    ##separating into training and test data\n",
    "    kfold = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    result = next(kfold.split(dummy), None)\n",
    "    \n",
    "    train = dummy.iloc[result[0]]\n",
    "    trainIn = train.drop(['days_trending'],axis=1)\n",
    "    trainOut = train['days_trending']\n",
    "    \n",
    "    test =  dummy.iloc[result[1]]\n",
    "    testIn = train.drop(['days_trending'],axis=1)\n",
    "    testOut = train['days_trending']\n",
    "    \n",
    "    ##Doing rf classifier \n",
    "    log = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    log.fit(trainIn,trainOut) \n",
    "    prediction = log.predict(testIn)\n",
    "    probs = log.predict_proba(testIn)\n",
    "    probs = probs[:, 1]\n",
    "    score = log.score(testIn,testOut)\n",
    "    print(\"Accuracy:\",\"\\n\",score)\n",
    "    print(\"Precision Score:\",average_precision_score(testOut,prediction),\"\\n\\n\")\n",
    "    print(\"ROC area under curve:\", roc_auc_score(testOut,probs))\n",
    "    print(\"Confusion Matrix:\",\"\\n\",confusion_matrix(testOut,prediction),\"\\n\")\n",
    "    \n",
    "##Grid Search For Random Forest Clasifier\n",
    "def RFClassifierCV(data):\n",
    "    ##Making boolean values of days trending more than avg. \n",
    "    dummy = data.copy()\n",
    "    mean = dummy['days_trending'].mean()\n",
    "\n",
    "    mask1 = dummy['days_trending'] >= mean\n",
    "    mask2 = dummy['days_trending'] < mean\n",
    "\n",
    "    dummy.loc[mask1, 'days_trending'] = 1\n",
    "    dummy.loc[mask2, 'days_trending'] = 0\n",
    "\n",
    "    ##separating into training and test data\n",
    "    kfold = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    result = next(kfold.split(dummy), None)\n",
    "    \n",
    "    train = dummy.iloc[result[0]]\n",
    "    trainIn = train.drop(['days_trending'],axis=1)\n",
    "    trainOut = train['days_trending']\n",
    "    \n",
    "    test =  dummy.iloc[result[1]]\n",
    "    testIn = train.drop(['days_trending'],axis=1)\n",
    "    testOut = train['days_trending']\n",
    "    \n",
    "    ##Doing random forest classifier with grid search\n",
    "    RF = RandomForestClassifier(random_state=0)\n",
    "    parameters = {\"n_estimators\":[25, 50, 100, 125,175,200], \n",
    "              \"max_depth\":list(range(1, 5)), \n",
    "              \"max_features\":['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    GS = GridSearchCV(RF, parameters, scoring='roc_auc', cv=5)\n",
    "    GS.fit(trainIn,trainOut)\n",
    "    bestRF = GS.best_estimator_\n",
    "    \n",
    "    ##Finding scoring parameters for best estimator\n",
    "    bestRF.fit(trainIn,trainOut) \n",
    "    prediction = bestRF.predict(testIn)\n",
    "    probs = bestRF.predict_proba(testIn)\n",
    "    probs = probs[:, 1]\n",
    "    score = bestRF.score(testIn,testOut)\n",
    "    #print(\"Best Estimator:\",\"\\n\",GS.best_estimator_)\n",
    "    print(\"ROC area under curve:\", roc_auc_score(testOut,probs))\n",
    "    print(\"Accuracy:\",\"\\n\",score)\n",
    "    print(\"Precision Score:\",average_precision_score(testOut,prediction),\"\\n\\n\")\n",
    "    print(\"Confusion Matrix:\",\"\\n\",confusion_matrix(testOut,prediction),\"\\n\")    \n",
    "        \n",
    "    bestFeatures = pd.DataFrame(bestRF.fit(trainIn,trainOut).feature_importances_).transpose()\n",
    "    bestFeatures.columns = list(testIn.columns.values)\n",
    "    bestFeatures = bestFeatures.transpose()\n",
    "    bestFeatures.sort_values(0, inplace=True, ascending=False)\n",
    "\n",
    "    return bestFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding \"days trending\" to the data set and grouping repeated videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of USA grouped data: (6455, 130)\n",
      "Size of GB grouped data: (3369, 130)\n",
      "Size of CA grouped data: (24573, 131)\n",
      "Size of MX grouped data: (33785, 130)\n"
     ]
    }
   ],
   "source": [
    "groupUSA = groupVideos(completeUSA)\n",
    "groupGB = groupVideos(completeGB)\n",
    "groupCA = groupVideos(completeCA)\n",
    "groupMX = groupVideos(completeMX)\n",
    "\n",
    "print('Size of USA grouped data:',groupUSA.shape)\n",
    "print('Size of GB grouped data:',groupGB.shape)\n",
    "print('Size of CA grouped data:',groupCA.shape)\n",
    "print('Size of MX grouped data:',groupMX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the algorithms to the updated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for logistic regression classifier:\n",
      "\n",
      "Accuracy: \n",
      " 0.6805194805194805\n",
      "Precision Score: 0.5047639755385419 \n",
      "\n",
      "\n",
      "ROC area under curve: 0.7478959431557467\n",
      "Confusion Matrix: \n",
      " [[1487  131]\n",
      " [ 730  347]] \n",
      "\n",
      "...........................................\n",
      "\n",
      "Results for random forest classifier:\n",
      "\n",
      "Accuracy: \n",
      " 0.6623376623376623\n",
      "Precision Score: 0.48095390706937424 \n",
      "\n",
      "\n",
      "ROC area under curve: 0.7549983185908758\n",
      "Confusion Matrix: \n",
      " [[1524   94]\n",
      " [ 816  261]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for logistic regression classifier:\\n\")\n",
    "LogisticReg(groupGB)\n",
    "\n",
    "print(\"...........................................\\n\")\n",
    "\n",
    "print(\"Results for random forest classifier:\\n\")\n",
    "RFClassifier(groupGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most predictive feature selection based on Recursive Feature Elimination (RFE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection for logistic regresion based on Recursive Feature Elimination (RFE)\n",
    "\n",
    "#(THIS TAKES FOREVER, so I included files with the results in the \"Data\" folder)\n",
    "model = LogisticRegresssion(penalty = \"l1\",solver='liblinear',max_iter=1000,C=0.1)\n",
    "nFeatures = 10\n",
    "\n",
    "RecursiveFeatureElimination(groupUSA,model,nFeatures)\n",
    "RecursiveFeatureElimination(groupGB,model,nFeatures)\n",
    "RecursiveFeatureElimination(groupCA,model,nFeatures)\n",
    "RecursiveFeatureElimination(groupMX,model,nFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting Best Features From Random Forest and Grid Search\n",
    "#(THIS TAKES FOREVER, so I included files with the results in the \"Data\" folder)\n",
    "\n",
    "bestFeaturesUSA = RFClassifierCV(groupUSA)\n",
    "bestFeaturesUSA.to_csv('Data/bestFeaturesUSA.csv')\n",
    "\n",
    "bestFeaturesGB = RFClassifierCV(groupGB)\n",
    "bestFeaturesGB.to_csv('Data/bestFeaturesGB.csv')\n",
    "\n",
    "bestFeaturesCA = RFClassifierCV(groupCA)\n",
    "bestFeaturesCA.to_csv('Data/bestFeaturesCA.csv')\n",
    "\n",
    "bestFeaturesMX = RFClassifierCV(groupMX)\n",
    "bestFeaturesMX.to_csv('Data/bestFeaturesMX.csv')\n",
    "\n",
    "bestFeaturesUSA.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample best features for continued video trendability in Mexico: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dislikes</td>\n",
       "      <td>0.230405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>views</td>\n",
       "      <td>0.221185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>likes</td>\n",
       "      <td>0.172999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment_count</td>\n",
       "      <td>0.147273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>percent_dislikes</td>\n",
       "      <td>0.027338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>likes/dislikes</td>\n",
       "      <td>0.021233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>percent_likes</td>\n",
       "      <td>0.018493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>percent_comments</td>\n",
       "      <td>0.014103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.008243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>category_id_Music</td>\n",
       "      <td>0.008197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>category_id_Comedy</td>\n",
       "      <td>0.007705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>0.006940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexico</td>\n",
       "      <td>0.006026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>likes/comments</td>\n",
       "      <td>0.005269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Music</td>\n",
       "      <td>0.004799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Resumen</td>\n",
       "      <td>0.004746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>video</td>\n",
       "      <td>0.004678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>category_id_Style</td>\n",
       "      <td>0.003004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0         0\n",
       "0             dislikes  0.230405\n",
       "1                views  0.221185\n",
       "2                likes  0.172999\n",
       "3        comment_count  0.147273\n",
       "4     percent_dislikes  0.027338\n",
       "5       likes/dislikes  0.021233\n",
       "6        percent_likes  0.018493\n",
       "7     percent_comments  0.014103\n",
       "8                    4  0.008243\n",
       "9    category_id_Music  0.008197\n",
       "10  category_id_Comedy  0.007705\n",
       "11             Twitter  0.006940\n",
       "12              mexico  0.006026\n",
       "13      likes/comments  0.005269\n",
       "14               Music  0.004799\n",
       "15             Resumen  0.004746\n",
       "16               video  0.004678\n",
       "17                   5  0.003348\n",
       "18                   1  0.003198\n",
       "19   category_id_Style  0.003004"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading most predictive features from the Data folder\n",
    "\n",
    "BestFeaturesUSA = pd.read_csv('Data/bestFeaturesUSA.csv', low_memory=False,error_bad_lines=False)\n",
    "BestFeaturesGB = pd.read_csv('Data/bestFeaturesGB.csv', low_memory=False,error_bad_lines=False)\n",
    "BestFeaturesCA = pd.read_csv('Data/bestFeaturesCA.csv', low_memory=False,error_bad_lines=False)\n",
    "BestFeaturesMX = pd.read_csv('Data/bestFeaturesMX.csv', low_memory=False,error_bad_lines=False)\n",
    "\n",
    "#I only list the most predictive features for Mexico here, but feel free to test around\n",
    "print(\"Sample best features for continued video trendability in Mexico: \")\n",
    "BestFeaturesMX.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Mexican videos are more likely to trend based on the number of user interactions they obtain (views, dislikes, likes, comments ext..). This should be obvious. However, we can also conclude that videos that contain 1)  Music, 2) Comedy, 3) Twitter mentions 4) and Mexico-related content are more likely to trend as well. Capitalization of the video has no effect on the continued trendability of the video. \n",
    "\n",
    "Please refer to the Report for a complete analysis for the remaining countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the top categories across countries (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category Prevalence in USA:\n",
      " Entertainment              9964\n",
      "Music                      6472\n",
      "Style                      4146\n",
      "Comedy                     3457\n",
      "Peaple and Blogs           3210\n",
      "News and Politics          2487\n",
      "Science and Tech.          2401\n",
      "Film and Animation         2345\n",
      "Sports                     2174\n",
      "Education                  1656\n",
      "Pets and Animals            920\n",
      "Gaming                      817\n",
      "Travel and Events           402\n",
      "Vehicles                    384\n",
      "Activism and Nonprofits      57\n",
      "Shows                        57\n",
      "Name: category_id, dtype: int64\n",
      "\n",
      "Category Prevalence in GB:\n",
      " Music                      13754\n",
      "Entertainment               9124\n",
      "Peaple and Blogs            2926\n",
      "Film and Animation          2577\n",
      "Style                       1928\n",
      "Sports                      1907\n",
      "Comedy                      1828\n",
      "Gaming                      1788\n",
      "News and Politics           1225\n",
      "Pets and Animals             534\n",
      "Science and Tech.            518\n",
      "Education                    457\n",
      "Vehicles                     144\n",
      "Travel and Events             96\n",
      "Activism and Nonprofits       90\n",
      "Shows                         20\n",
      "Name: category_id, dtype: int64\n",
      "\n",
      "Category Prevalence in CA:\n",
      " Entertainment              13451\n",
      "News and Politics           4159\n",
      "Peaple and Blogs            4105\n",
      "Comedy                      3773\n",
      "Music                       3731\n",
      "Sports                      2787\n",
      "Film and Animation          2060\n",
      "Style                       2007\n",
      "Gaming                      1344\n",
      "Science and Tech.           1155\n",
      "Education                    991\n",
      "Travel and Events            392\n",
      "Pets and Animals             369\n",
      "Vehicles                     353\n",
      "Shows                        124\n",
      "Activism and Nonprofits       74\n",
      "Movies                         6\n",
      "Name: category_id, dtype: int64\n",
      "\n",
      "Category Prevalence in MX:\n",
      " Entertainment              13487\n",
      "Peaple and Blogs            8159\n",
      "Sports                      4050\n",
      "Music                       3371\n",
      "News and Politics           3113\n",
      "Style                       2467\n",
      "Comedy                      1742\n",
      "Film and Animation          1298\n",
      "Gaming                       994\n",
      "Education                    532\n",
      "Science and Tech.            531\n",
      "Vehicles                     252\n",
      "Activism and Nonprofits      252\n",
      "Travel and Events            117\n",
      "Pets and Animals              83\n",
      "Shows                          3\n",
      "Name: category_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nCategory Prevalence in USA:\\n',dataUSA.category_id.value_counts())\n",
    "print('\\nCategory Prevalence in GB:\\n',dataGB.category_id.value_counts())\n",
    "print('\\nCategory Prevalence in CA:\\n',dataCA.category_id.value_counts())\n",
    "print('\\nCategory Prevalence in MX:\\n',dataMX.category_id.value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
